---
title: "Feature Selection Methods Tutorial"
author: "Craig McLean"
date: "27 November 2018"
output: 
  html_document: 
    fig_height: 7
    fig_width: 10
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Feature Selection, Dimensionality reduction, and Random Forests

Feature Selection is a process of selecting a subset of relevant features for use in a classification problem. This allows for :

1. Simplification of Models
2. Shorter training times
3. Avoiding overfitting, so that models can be generalised to other incoming data

Dimensionality reduction is a way of reducing the number of random variables under consideration. In this project, we will be using PCA to map the data to a 2-Dimensional frame along the eigen vectors that obtain the maximum variance. For classification, we will be using the random forests algorithm. This method classifies data with the help of a multitude of decision trees at training time and outputting the class that is the mode of the classes.

## Reading relevant libraries and data

We will load the necessary libraries and tools for data analysis.

```{r, set up}
library(tidyverse)
library(ROCR)
library(caret)
library(doParallel)
library(ellipse)
voice <- read.csv(here::here("data/2018-11-27/voice.csv"),
                  header=T,stringsAsFactors = F)
str(voice)
voice$label <- as.factor(voice$label)
plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 10,colour = "black",hjust=0.5),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=8),
    axis.text = element_text(size=8),
    axis.title.x = element_text(hjust=1),
    axis.title.y = element_text(hjust=1),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "bold"),
    legend.text = element_text(colour = "black", face = "bold"))
}


auc_values <- data.frame(feature_selection = c("No Feature Elimination","Correlation","Recursive Feature Elimination"),values=c(0,0,0))
```

The data is made of 3168 observations (rows) and 21 variables (columns). The last variable denotes the class to which each row belongs. We convert it from a character variable to a `factor` variable for building the model.

## How many instances of each class (male/female) are there?

Counting and visualizing the number of male and female observations w/in the data.

```{r}
voice %>% group_by(label) %>%
  summarise(n=n()) %>%
  ggplot(aes(x=label,y=n))+
  geom_bar(stat="identity")+plotTheme()+labs(title="Number of Each Instance")
```

## How does each numerical variable vary across the labels?

Visualizing the distribution of each variable with respect to each factor.

```{r}
voice %>% na.omit() %>%
  gather(type,value,1:20) %>% ## see how gather works
  ggplot(aes(x=value,fill=label)) + 
    geom_density(alpha=0.3)+
    plotTheme()+
    facet_wrap(~type,scales="free")+
    theme(axis.text.x = element_text(angle = 90,vjust=1))+
    labs(title="Density Plots of Data across Variables")
```

The above plot shows that for all features, there is a bit of overlap between the values for the data that is labelled male and female. The only exceptions are sd, Q25, meanfun, sfm, and IQR.

## Principal Components Analysis

Principal components analysis is a statistical procedure that uses an orthogonal tranformation to convert data to a set of linearly uncorrelated variables.

```{r}
#library(pcaGoPromoter)

pca_viz <- function(dataframe,label,heading) {
    
  data_new <- dataframe[-label] ## removing factor col
  #head(data_new)
  
  pca_temp <- prcomp((data_new),scale=T,center=T)
  
  pcaOutput_df <- as.data.frame(pca_temp$x)
  pcaOutput_df$label <- dataframe[,label]
  
  ## Centrolids just represents the mean of one class of observations 
  ## and is returned in the form of a cartesian coordinate
  centroids <- pcaOutput_df %>% 
      group_by(label) %>%
    summarise(PC1=mean(PC1),PC2=mean(PC2))
  # 95% confidence region 
  
  conf.rgn_male <- data.frame(label="male",ellipse(cov(pcaOutput_df[pcaOutput_df$label == "male", c("PC1","PC2")]), ## covariance of PC1 and PC2 with respect to male
                         centre = as.matrix(centroids[centroids$label == "male", c("PC1","PC2")]),
                         ## cartesian coordinates of the centers
                         level = 0.95))
  conf.rgn_female <- data.frame(label="female",ellipse(cov(pcaOutput_df[pcaOutput_df$label == "female", c("PC1","PC2")]),
                         centre = as.matrix(centroids[centroids$label == "female", c("PC1","PC2")]),
                         level = 0.95))
  
  conf.rgn <- bind_rows(conf.rgn_female,conf.rgn_male) %>% mutate(label = as.factor(label))
  var <- (pca_temp$sdev)^2/sum((pca_temp$sdev)^2)

  pcaOutput_df %>%
    ggplot(aes(x=PC1,y=PC2,colour=label))+
    geom_point(alpha=0.3) +
    geom_polygon(data=conf.rgn,aes(fill=label),alpha=0.2) + ## drawing the elipses
    labs(title=heading,
         x=paste("PC1",round(var[1]*100,2),"% Variance"),
         y = paste("PC2",round(var[2]*100,2),"% Variance"))+
      plotTheme()+
      theme(legend.position = "bottom")
}

pca_viz(dataframe = voice, label = 21, 
        heading = "PCA Visualization") ## 21 is the factor column for gender

```

We see an overlap between the 95% confidence regions of the __male__ and __female__ clusters.

## Feature Importance

```{r}
registerDoParallel()

## control behavior of train function
control <- trainControl(method="repeatedcv",number=10,repeats = 10)

## fit predictive model of different tuning parameters
start_time <- Sys.time()
## Note, I am doing this on the original data w/out PCA
model <- train(label ~ ., data = voice, ## training data with respect to params
               method = "rf", ## running a random forest
               preProcess = c("scale", "center"), ## normalization stuff...
               trControl = control)
end_time <- Sys.time()
beepr::beep()

## varImp - calculates the importance of a sample to producing differences between datasets
importance<- varImp(model,scale=T)
imp_df1 <- importance$importance
imp_df1$group <- rownames(imp_df1)

imp_df1 %>%
  ggplot(aes(x=reorder(group,Overall),y=Overall),size=2)+
    geom_bar(stat = "identity")+
    theme(axis.text.x = element_text(vjust=1,angle=90))+
    labs(x="Variable",y="Overall Importance",title="Scaled Feature Importance")+
  plotTheme()

```

Variable importance represents the importance in the sample to calculate the decision tree. 

According to the plot above, `meanfun`,`IQR`,`Q25` are the three most important attributes in the dataset. This is used as a way to understand our dataset. 

This gives a numeric way to show what specific variables are most important in the data.

### Random forests

Running the random forest on a subset of the data to then cross validate. 

```{r}
set.seed(100)
## selecting about 90% of the data to train the model
train <- sample(dim(voice)[1],dim(voice)[1]*0.9)

# subsetting the data to train the random forrest
voice_train <- voice[train,]
voice_test <- voice[-train,]

set.seed(100)
model_rf <- train(label~.,
                  data=voice_train,
                  method="rf",
            preProcess=c("scale","center"),
            trControl = trainControl(method = "repeatedcv",number=5,repeats = 10,verboseIter = F)
                  )
```

### ROC Curves

```{r}

voice_test_pred <- predict(model_rf, voice_test[,-21]) # predicts which sample belongs to either group
predvec <- ifelse(voice_test_pred=="female", 1, 0)
realvec <- ifelse(voice_test$label=="female", 1, 0)

## Checking how well the model works
pred <- prediction(predvec,realvec)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main = "ROC curve for Random Forest Classifier",col = "blue", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
perf.auc <- performance(pred, measure = "auc")
paste("ROC Value",unlist(perf.auc@y.values))

auc_values[1,]$values <- unlist(perf.auc@y.values)
```


## Removing Correlated Variables 

Highly correlated features provide redundant information. By removing them, we can avoid any kind of bias associated with the prediction step. In this section we find pair wise correlations using the `cor` function. The correlation plots are visualized using the `geom_tile()` function.

```{r}
library(reshape2)
cor_voice <- round(cor(voice[,-21]),2)
cor_voice_df <- melt((cor_voice))
cor_voice_df %>%
  ggplot(aes(x=Var1,y=Var2,fill=value))+geom_tile()+plotTheme()+theme(axis.text.x = element_text(vjust=1,angle=90),legend.position = "bottom")+scale_fill_continuous(low="#eb0096",high="#a3742c")+labs(title="Correlation Plots")


```

We will define a high correlation as greater than equal to 0.7.


```{r}
highlyCor <- colnames(voice)[findCorrelation(cor_voice, cutoff = 0.7, verbose = TRUE)]
print(highlyCor)



```

The columns that need to be removed to avoid a predictive bias are as above.

**Removing the columns**

```{r}

voice_correlation <- voice[,which(!colnames(voice) %in% highlyCor)]

```

9 variables that were highly correlated were removed.

### PCA Visualization 



```{r}
pca_viz(voice_correlation,which(colnames(voice_correlation)=="label"),"PCA with Correlated Terms Removed")
```

With correlated variables in the data set, the first component contributed around 45% of the total variance in the data set. Upon removal of correlated variables, the contribution to the variance reduces to 31%. Correlated variables tend to overemphasize the contribution of the principal component to the total variance. For a detailed explanantion please read this [article](http://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca)


### Training The random forest

Now that we have removed the correlated terms, we will apply the random forest algorithm

```{r}
train <- sample(dim(voice)[1],dim(voice)[1]*0.9)

voice_train <- voice_correlation[train,]
voice_test <- voice_correlation[-train,]

set.seed(100)

model_rf <- train(label~.,
                  data=voice_train,
                  method="rf",
            preProcess=c("scale","center"),
            trControl = trainControl(method = "repeatedcv",number=5,repeats = 10,verboseIter = F)
                  )

voice_test_pred <- predict(model_rf, voice_test[,-which(colnames(voice_test)=="label")])

```

### ROC Curves



```{r}
predvec <- ifelse(voice_test_pred=="female", 1, 0)
realvec <- ifelse(voice_test$label=="female", 1, 0)
pred <- prediction(predvec,realvec)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main = "ROC curve for Random Forest Classifier With Correlated Terms Removed",col = "blue", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
perf.auc <- performance(pred, measure = "auc")
paste("ROC Value",unlist(perf.auc@y.values))
auc_values[2,]$values <- unlist(perf.auc@y.values)
```


## Recursive Feature Elimination

```{r}
set.seed(100)
control <- rfeControl(functions=rfFuncs,method = "cv",number=10)
results_1 <- rfe(x=voice[,-21],y=voice$label,sizes=(1:9),rfeControl = control)

predictors(results_1)
```

### PCA Visualization


```{r}
## only looking at selected features between samples
voice_rfe <- voice[,c(which(colnames(voice) %in% predictors(results_1)),21)]
pca_viz(voice_rfe,which(colnames(voice_rfe)=="label"),"PCA with rfe")
```



### Random Forest Classification


```{r}
voice_train <- voice_rfe[train,]
voice_test <- voice_rfe[-train,]

set.seed(100)

model_rf <- train(label~.,
                  data=voice_train,
                  method="rf",
            preProcess=c("scale","center"),
            trControl = trainControl(method = "repeatedcv",number=5,repeats = 10,verboseIter = F)
                  )

voice_test_pred <- predict(model_rf, voice_test[,-which(colnames(voice_test)=="label")])
```


### ROC Curves and AUC


```{r}
predvec <- ifelse(voice_test_pred=="female", 1, 0)
realvec <- ifelse(voice_test$label=="female", 1, 0)
pred <- prediction(predvec,realvec)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main = "ROC curve for Random Forest Classifier",col = "blue", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
perf.auc <- performance(pred, measure = "auc")
paste("ROC Value",unlist(perf.auc@y.values))

auc_values[3,]$values <- unlist(perf.auc@y.values)
```

## Comparing AUC Values and Feature Selection Methods

```{r}
auc_values %>%
  ggplot(aes(x=feature_selection,y=values))+geom_bar(stat="identity")+plotTheme()+theme(axis.text.x = element_text(vjust=-1,angle=90))
```